{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio python-dotenv lightning geopandas pandas sqlalchemy psycopg2-binary openpyxl geoalchemy2 python-dotenv numpy mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/utils/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/utils/models.py\n",
    "from torch import nan_to_num\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn.functional import mse_loss, binary_cross_entropy\n",
    "from torch.nn import Sigmoid, LeakyReLU, Linear, ModuleList, BatchNorm1d, Tanh\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics import MeanSquaredError, R2Score\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "from utils.target_types import DTarget\n",
    "\n",
    "\n",
    "\n",
    "class STASGeneralModel(LightningModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_features:int, \n",
    "            target_type:DTarget,\n",
    "            relu_slope:float=0.01, \n",
    "            learning_rate:float=0.01,\n",
    "            multiplyer:int=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # parameter intilization\n",
    "        self.relu_slope = relu_slope\n",
    "        self.target_type = target_type\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # init metrics\n",
    "        if self.target_type == DTarget.BOOLEAN:\n",
    "            self.f1_score = BinaryF1Score()\n",
    "        elif self.target_type == DTarget.AREA:\n",
    "            self.rmse = MeanSquaredError(squared=False)\n",
    "            self.r2_score = R2Score()\n",
    "\n",
    "        # build layers\n",
    "        self.model = ModuleList()\n",
    "        self.__add_linear_hidden_block(\n",
    "            in_features=int(1.00*num_features),\n",
    "            out_features=int(1.00*num_features*multiplyer)\n",
    "        ),\n",
    "        self.__add_linear_hidden_block(\n",
    "            in_features=int(1.00*num_features*multiplyer),\n",
    "            out_features=int(0.75*num_features*multiplyer)\n",
    "        ),\n",
    "        self.__add_linear_hidden_block(\n",
    "            in_features=int(0.75*num_features*multiplyer),\n",
    "            out_features=int(0.50*num_features*multiplyer)\n",
    "        ),\n",
    "        self.__add_linear_hidden_block(\n",
    "            in_features=int(0.50*num_features*multiplyer),\n",
    "            out_features=int(0.25*num_features*multiplyer)\n",
    "        ),\n",
    "        self.__add_output_block(\n",
    "            in_features=int(0.25*num_features*multiplyer)\n",
    "        )\n",
    "    \n",
    "    def __add_linear_hidden_block(self, in_features, out_features):\n",
    "        self.model.append(\n",
    "            Linear(\n",
    "                in_features=in_features,\n",
    "                out_features=out_features,\n",
    "            )\n",
    "        )\n",
    "        self.model.append(\n",
    "            LeakyReLU(negative_slope=self.relu_slope)\n",
    "        )\n",
    "        self.model.append(\n",
    "            BatchNorm1d(out_features)\n",
    "        )\n",
    "        return \n",
    "    \n",
    "    def __add_output_block(self, in_features):\n",
    "        self.model.append(\n",
    "            Linear(\n",
    "                in_features=in_features,\n",
    "                out_features=1,\n",
    "            )\n",
    "        )\n",
    "        if self.target_type == DTarget.BOOLEAN:\n",
    "            self.model.append(Sigmoid())\n",
    "        # elif self.target_type == DTarget.AREA:\n",
    "        #     self.model.append(Tanh())\n",
    "            # self.model.append(LeakyReLU(negative_slope=self.relu_slope))\n",
    "\n",
    "        return \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "            \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch[0])\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        if self.target_type == DTarget.BOOLEAN:\n",
    "            loss = binary_cross_entropy(y_hat, y)\n",
    "        elif self.target_type == DTarget.AREA:\n",
    "            loss = mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(nan_to_num(x))\n",
    "        if self.target_type == DTarget.BOOLEAN:\n",
    "            loss = binary_cross_entropy(y_hat, y)\n",
    "        elif self.target_type == DTarget.AREA:\n",
    "            loss = mse_loss(y_hat, y)\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        if self.target_type == DTarget.BOOLEAN:\n",
    "            self.f1_score(y_hat, y)\n",
    "            self.log('f1_score', self.f1_score, on_epoch=True)\n",
    "        elif self.target_type == DTarget.AREA:\n",
    "            self.rmse(y_hat, y)\n",
    "            self.r2_score(y_hat, y)\n",
    "            self.log('rmse', self.rmse, on_epoch=True)\n",
    "            self.log('r2_score', self.r2_score, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def set_lr(self, lr:float) -> None:\n",
    "        self.learning_rate = lr\n",
    "        self.configure_optimizers()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.target_type == DTarget.BOOLEAN:\n",
    "            optimizer = SGD(self.parameters(), lr=self.learning_rate)\n",
    "        elif self.target_type == DTarget.AREA:\n",
    "            optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/utils/fed_node.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/utils/fed_node.py\n",
    "from pandas import DataFrame\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from utils.dataset import Dataset\n",
    "from utils.stas import STASDataGenerator\n",
    "from utils.target_types import DTarget\n",
    "from utils.models import STASGeneralModel\n",
    "\n",
    "class FederatedNode():\n",
    "    def __init__(\n",
    "            self, \n",
    "            _id:int,\n",
    "            d:DataFrame,\n",
    "            f:DataFrame,\n",
    "            d_type:Dataset,\n",
    "            d_target:DTarget,\n",
    "            n:int,\n",
    "            m:int,\n",
    "            k:int=1,\n",
    "            learning_rate:float=0.001, \n",
    "            batch_size:int=2048,\n",
    "            workers:int=4,\n",
    "            multiplyer:int=1,\n",
    "    ):\n",
    "        self.id = _id\n",
    "        print(f\"Initializing id: {self.id:3}\")\n",
    "\n",
    "        # init stas sampler\n",
    "        stas_sampler = STASDataGenerator(\n",
    "            d=d,\n",
    "            f=f,\n",
    "            k=k,\n",
    "            n=n,\n",
    "            m=m,\n",
    "            d_type=d_type,\n",
    "            d_target=d_target,\n",
    "        )\n",
    "\n",
    "        # build train and test data\n",
    "        train_data = TensorDataset(\n",
    "            stas_sampler.train_x,\n",
    "            stas_sampler.train_y\n",
    "        )\n",
    "        test_data = TensorDataset(\n",
    "            stas_sampler.test_x,\n",
    "            stas_sampler.test_y\n",
    "        )\n",
    "        del stas_sampler\n",
    "\n",
    "        # build dataloaders\n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset=train_data,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.test_dataloader = DataLoader(\n",
    "            dataset=test_data,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "        del train_data\n",
    "        del test_data\n",
    "\n",
    "        # build model\n",
    "        feature_count = (n+1) * len(d_type.value['data_columns'])\n",
    "        self.model = STASGeneralModel(\n",
    "            num_features=feature_count, \n",
    "            target_type=d_target,\n",
    "            learning_rate=learning_rate,\n",
    "            multiplyer=multiplyer,\n",
    "        )\n",
    "\n",
    "        # print(f\"Initialization Finished for id: {self.id:3}\")\n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        self.model.set_lr(lr=lr)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model:STASGeneralModel):\n",
    "        self.model = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/utils/fed_env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/utils/fed_env.py\n",
    "from sqlalchemy.engine import URL\n",
    "from numpy.random import choice\n",
    "\n",
    "from utils.generate_subdivision import GenSubdivision\n",
    "from utils.models import STASGeneralModel\n",
    "from utils.target_types import DTarget\n",
    "from utils.fed_node import FederatedNode\n",
    "from utils.dataset import Dataset\n",
    "\n",
    "import mlflow\n",
    "from copy import deepcopy\n",
    "from torch import no_grad, cat\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# from torch import set_float32_matmul_precision\n",
    "# set_float32_matmul_precision('high')\n",
    "\n",
    "\n",
    "class FederatedEnvironment():\n",
    "    def __init__(\n",
    "            self, \n",
    "            d_type:Dataset,\n",
    "            d_target:DTarget,\n",
    "            db_url:URL,\n",
    "            mlflow_uri:str,\n",
    "            n:int,\n",
    "            m:int,\n",
    "            k:int=1, \n",
    "            learning_rate:float=0.001,\n",
    "            batch_size:float=2048,\n",
    "            multiplyer:int=1\n",
    "    )->None:\n",
    "\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.nodes = []\n",
    "\n",
    "        self.d_type = d_type\n",
    "        self.d_target = d_target\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.multiplyer = multiplyer\n",
    "\n",
    "        # load D \n",
    "        d_data_generator = GenSubdivision(\n",
    "            d_full = self.d_type,\n",
    "            db_url = db_url,\n",
    "            n = self.n,\n",
    "            m = self.m,\n",
    "            k = self.k,\n",
    "        )\n",
    "        d_map = d_data_generator.gen_subdivisions()\n",
    "        del d_data_generator\n",
    "\n",
    "        # load F\n",
    "        fire_generator = GenSubdivision(\n",
    "            d_full = Dataset.FIRE,\n",
    "            db_url = db_url,\n",
    "            n = self.n,\n",
    "            m = self.m,\n",
    "            k = self.k,\n",
    "        )\n",
    "        fire_d_map = fire_generator.gen_subdivisions()\n",
    "        del fire_generator\n",
    "\n",
    "        # build nodes\n",
    "        for (d_s_id, d), (f_s_id, f) in zip(d_map,fire_d_map):\n",
    "            assert d_s_id == f_s_id, F\"D and F are not zipped with same S id\"\n",
    "            node = FederatedNode(\n",
    "                _id=d_s_id,\n",
    "                d=d,\n",
    "                f=f,\n",
    "                d_type=self.d_type,\n",
    "                d_target=d_target,\n",
    "                n=n,\n",
    "                m=m,\n",
    "                k=k,\n",
    "                learning_rate=self.learning_rate, \n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "            self.nodes.append(node)\n",
    "\n",
    "        # init mlflow\n",
    "        self.mlflow_uri = mlflow_uri\n",
    "        self.__init_mlflow__()\n",
    "\n",
    "        # init global model \n",
    "        self.reset_global_model()\n",
    "\n",
    "    def set_lr(self, lr:float) -> None:\n",
    "        self.learning_rate = lr\n",
    "        self.__reset_models()\n",
    "\n",
    "    \n",
    "    def get_mlflow_exp_name(self):\n",
    "        return f\"COMPSAC_24_FL_{self.d_type.name}_{self.d_target.name}\"\n",
    "\n",
    "    def __init_mlflow__(self):        \n",
    "        mlflow.set_tracking_uri(self.mlflow_uri)\n",
    "        mlflow.set_experiment(self.get_mlflow_exp_name())\n",
    "\n",
    "    def get_num_features(self):\n",
    "        return (self.n+1) * len(self.d_type.value['data_columns'])\n",
    "\n",
    "    def reset_global_model(self)->None:\n",
    "        feature_count = self.get_num_features()\n",
    "        self.global_model = STASGeneralModel(\n",
    "            num_features=feature_count, \n",
    "            target_type=self.d_target,\n",
    "            learning_rate=self.learning_rate,\n",
    "            multiplyer=self.multiplyer\n",
    "        )\n",
    "\n",
    "    def get_global_test_loader(self):\n",
    "        test_x = []\n",
    "        test_y = []\n",
    "        num_nodes = self.get_node_count()\n",
    "        for node_index in range(num_nodes):\n",
    "            node = self.nodes[node_index]\n",
    "            node_test_x = node.test_dataloader.dataset.tensors[0]\n",
    "            node_test_y = node.test_dataloader.dataset.tensors[1]\n",
    "            test_x.append(node_test_x)\n",
    "            test_y.append(node_test_y)\n",
    "        del num_nodes\n",
    "        del node\n",
    "        del node_test_x\n",
    "        del node_test_y\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            cat(test_x, dim = 0),\n",
    "            cat(test_y, dim = 0)\n",
    "        )\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        del dataset\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def get_phase_3_metrics(self)->dict:\n",
    "        print(\"Phase 3 testing\")\n",
    "        # create lighting trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=1, \n",
    "            accelerator=\"gpu\", \n",
    "            enable_checkpointing=False,\n",
    "            enable_progress_bar=True, \n",
    "            logger=False,\n",
    "            callbacks=[]\n",
    "        )\n",
    "\n",
    "        # run validation loop on trainer\n",
    "        metrics = trainer.validate(\n",
    "            model=self.global_model, \n",
    "            dataloaders=self.get_global_test_loader()\n",
    "        )\n",
    "        return_metrics = {}\n",
    "        for metric in metrics[0].keys():\n",
    "            return_metrics[f\"Phase_3_{metric}\"] = metrics[0][metric]\n",
    "        del trainer\n",
    "        del metrics\n",
    "        return return_metrics\n",
    "\n",
    "    def get_phase_2_metrics(self, node_indexes:list):\n",
    "        print(\"Phase 2 testing\")\n",
    "        metrics = {} \n",
    "        # get global train loader\n",
    "        global_test_loader = self.get_global_test_loader()\n",
    "        # get metrics for each node in list \"node_indexes\"\n",
    "        for node_index in node_indexes:\n",
    "            # create trainer\n",
    "            trainer = Trainer(\n",
    "                max_epochs=1, \n",
    "                accelerator=\"gpu\", \n",
    "                enable_checkpointing=False,\n",
    "                check_val_every_n_epoch=1, \n",
    "                logger=False, \n",
    "                callbacks=[],\n",
    "                enable_progress_bar=False\n",
    "            )\n",
    "            node = self.nodes[node_index]\n",
    "            print(f\"Phase 2 testing node {node.id}\")\n",
    "            node_metrics = trainer.validate(\n",
    "                model=node.model, \n",
    "                dataloaders=global_test_loader\n",
    "            )\n",
    "            for metric in node_metrics[0].keys():\n",
    "                metrics[f\"Phase_2_{metric}_{node.id}\"] = node_metrics[0][metric]\n",
    "        \n",
    "        del global_test_loader\n",
    "        del trainer\n",
    "        return metrics\n",
    "\n",
    "    def get_phase_1_metrics(self, node_index:int):\n",
    "        print(f\"Phase 1 testing in node {node_index}\")\n",
    "        # get node\n",
    "        node = self.nodes[node_index]\n",
    "        # create lighting trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=1, \n",
    "            accelerator=\"gpu\", \n",
    "            enable_progress_bar=True, \n",
    "            logger=False,\n",
    "            callbacks=[]\n",
    "        )\n",
    "        # run validation loop on trainer\n",
    "        metrics = trainer.validate(\n",
    "            model=node.model, \n",
    "            dataloaders=node.test_dataloader\n",
    "        )\n",
    "        return_metrics = {}\n",
    "        for metric in metrics[0].keys():\n",
    "            return_metrics[f\"Phase_1_{metric}_{node_index}\"] = metrics[0][metric]\n",
    "        del trainer\n",
    "        del metrics\n",
    "        return return_metrics\n",
    "\n",
    "    def get_global_model(self)->STASGeneralModel:\n",
    "        feature_count = self.get_num_features()\n",
    "        # creating a default copy of the model \n",
    "        model = STASGeneralModel(\n",
    "            num_features=feature_count, \n",
    "            target_type=self.d_target,\n",
    "            learning_rate=self.learning_rate,\n",
    "            multiplyer=self.multiplyer\n",
    "        )\n",
    "        # loading a DEEPCOPY of the state dict into the model\n",
    "        model.load_state_dict(deepcopy(self.global_model.state_dict()))\n",
    "        return model\n",
    "    \n",
    "    def get_nodes(self, node_indexes:list)->list:\n",
    "        return [self.nodes[index] for index in node_indexes]\n",
    "\n",
    "    def get_node_count(self)->int:\n",
    "        return len(self.nodes)\n",
    "\n",
    "    def randomly_sample_nodes(self, n:int)->list:\n",
    "        num_nodes = self.get_node_count()\n",
    "        node_indexes = list(range(num_nodes))\n",
    "        selected_node_index = choice(\n",
    "            node_indexes, \n",
    "            n, \n",
    "            replace=False\n",
    "        )\n",
    "        return list(selected_node_index)\n",
    "\n",
    "    def __reset_models(self)->None:\n",
    "        # reset global model\n",
    "        self.reset_global_model()\n",
    "\n",
    "        node_indexes = list(range(len(self.nodes)))\n",
    "        # update clinets to global model\n",
    "        self.update_node_models(node_indexes)\n",
    "\n",
    "\n",
    "    def train_nodes(\n",
    "            self, \n",
    "            node_indexes:list, \n",
    "            epochs:int=20, \n",
    "            reset:bool=False, \n",
    "            log:bool=False\n",
    "    )->None:\n",
    "        if reset:\n",
    "            # reset all models\n",
    "            self.__reset_models()\n",
    "        \n",
    "        # train all node models\n",
    "        for node_index in node_indexes:\n",
    "            # get node\n",
    "            node = self.nodes[node_index]\n",
    "\n",
    "            print(f\"\\n++++++++++++++++++++++++++   Training node {node.id:2.0f}   +++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "            # Initialize a trainer\n",
    "            trainer = Trainer(\n",
    "                max_epochs=epochs, \n",
    "                accelerator=\"gpu\", \n",
    "                check_val_every_n_epoch=1, \n",
    "                enable_checkpointing=False,\n",
    "                logger=False, \n",
    "                enable_progress_bar=True,\n",
    "                callbacks=[]\n",
    "                )\n",
    "\n",
    "            if log:\n",
    "                # Auto log all MLflow entities\n",
    "                mlflow.pytorch.autolog()\n",
    "                with mlflow.start_run():\n",
    "                    # set model tages\n",
    "                    mlflow.set_tags(\n",
    "                        {\n",
    "                            'is_node':True, \n",
    "                            'node_id': node.id, \n",
    "                            'target': self.d_target.name,\n",
    "                            'd_type': self.d_type.name,\n",
    "                            'train_size': len(node.train_dataloader.dataset),\n",
    "                            'test_size': len(node.test_dataloader.dataset),\n",
    "                            'multiplyer': self.multiplyer,\n",
    "                            'n': self.n,\n",
    "                            'm': self.m,\n",
    "                            'k': self.k,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    mlflow.log_params({\n",
    "                        'lr': self.learning_rate, \n",
    "                        'batch_size': self.batch_size \n",
    "                    })\n",
    "\n",
    "                    # fit model\n",
    "                    trainer.fit(\n",
    "                        model=node.model, \n",
    "                        train_dataloaders=node.train_dataloader, \n",
    "                        val_dataloaders=node.test_dataloader\n",
    "                    )\n",
    "\n",
    "                    # log dataset\n",
    "                    mlflow.log_input(\n",
    "                        dataset=mlflow.data.from_numpy(node.test_dataloader.dataset.tensors[0].numpy()),\n",
    "                        context='testing_x'\n",
    "                    )\n",
    "\n",
    "                    mlflow.log_input(\n",
    "                        dataset=mlflow.data.from_numpy(node.test_dataloader.dataset.tensors[1].numpy()),\n",
    "                        context='testing_y'\n",
    "                    )\n",
    "\n",
    "                    # log model\n",
    "                    mlflow.pytorch.log_model(node.model, \"model\")\n",
    "                    \n",
    "                    # end mlflow logging\n",
    "                    mlflow.end_run()\n",
    "            else:\n",
    "                # only run training \n",
    "                trainer.fit(model=node.model, train_dataloaders=node.train_dataloader)\n",
    "            del trainer\n",
    "\n",
    "    def update_node_models(self, indexes)->None:\n",
    "        print(\"\\nStarted updating nodes\")\n",
    "        for index in indexes:\n",
    "            node = self.nodes[index]\n",
    "\n",
    "            global_model = self.get_global_model()\n",
    "\n",
    "            node.set_model(global_model)\n",
    "            print(f\"Updating node {node.id}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def aggregrate_global_model(self, node_indexes:list):\n",
    "        for layer_index, __layer in enumerate(self.global_model.model):\n",
    "            try:\n",
    "                # load layer 'weights' and 'bias'\n",
    "                gloabl_layer_weight, gloabl_layer_bias = self.global_model.model[layer_index].parameters()\n",
    "\n",
    "                with no_grad():\n",
    "                    # average weights and bias for all node models\n",
    "                    for node_index in node_indexes:\n",
    "                        node_model = self.nodes[node_index].model\n",
    "                        node_layer_weight, node_layer_bias = node_model.model[layer_index].parameters()\n",
    "                        gloabl_layer_weight += node_layer_weight\n",
    "                        gloabl_layer_bias += node_layer_bias\n",
    "                    gloabl_layer_weight /= (len(node_indexes) + 1)\n",
    "                    gloabl_layer_bias /= (len(node_indexes) + 1) \n",
    "                \n",
    "                # set gradient to True for future training\n",
    "                gloabl_layer_weight.requires_grad = True\n",
    "                gloabl_layer_bias.requires_grad = True\n",
    "            except Exception as e:\n",
    "                print(f\"{__layer} --> {e}\")\n",
    "                continue\n",
    "\n",
    "    def fed_avg(\n",
    "            self, \n",
    "            epochs:int=1000, \n",
    "            num_nodes_per_epoch:int=2,\n",
    "            num_training_per_epoch:int=1\n",
    "    )->None:\n",
    "        # reset global model \n",
    "        self.reset_global_model()\n",
    "        \n",
    "        # set up mlflow tracking\n",
    "        with mlflow.start_run():\n",
    "            # tag run\n",
    "            mlflow.set_tags({\n",
    "                'is_node':False, \n",
    "                'node_id': -1, \n",
    "                'target': self.d_target.name,\n",
    "                'd_type': self.d_type.name,\n",
    "                'train_size': sum([len(node.train_dataloader.dataset)for node in self.nodes]),\n",
    "                'test_size': sum([len(node.test_dataloader.dataset)for node in self.nodes]),\n",
    "                'num_nodes_per_epoch': num_nodes_per_epoch,\n",
    "                'num_training_per_epoch':num_training_per_epoch,\n",
    "                'multiplyer': self.multiplyer,\n",
    "                'n': self.n,\n",
    "                'm': self.m,\n",
    "                'k': self.k,\n",
    "            })\n",
    "            \n",
    "            mlflow.log_params({\n",
    "                'lr': self.learning_rate,\n",
    "                'batch_size': self.batch_size \n",
    "            })\n",
    "\n",
    "            # start fed average training\n",
    "            for epoch in range(int(epochs//num_training_per_epoch)):\n",
    "                # select nodes \n",
    "                node_indexes = self.randomly_sample_nodes(n=num_nodes_per_epoch)\n",
    "\n",
    "                # update all node models\n",
    "                self.update_node_models(indexes=node_indexes)\n",
    "                \n",
    "                # train nodes \n",
    "                self.train_nodes(\n",
    "                    node_indexes=node_indexes, \n",
    "                    epochs=num_training_per_epoch\n",
    "                )\n",
    "\n",
    "                # update global model weight using fed average\n",
    "                self.aggregrate_global_model(node_indexes=node_indexes)\n",
    "                \n",
    "                # get global model metrics\n",
    "                global_metrics = self.get_phase_3_metrics()\n",
    "                mlflow.log_metrics(metrics=global_metrics, step=epoch)\n",
    "\n",
    "                # get local model metrics on global data\n",
    "                all_node_index = list(range(self.get_node_count()))\n",
    "                node_metrics = self.get_phase_2_metrics(all_node_index)\n",
    "                mlflow.log_metrics(metrics=node_metrics, step=epoch)\n",
    "\n",
    "            # log models\n",
    "            mlflow.pytorch.log_model(self.get_global_model(), \"model\")\n",
    "\n",
    "            # end mlflow run\n",
    "            mlflow.end_run()\n",
    "        \n",
    "    def run_phase_1(self, epochs:int=20):\n",
    "        print(F\"Started Phase 1...\")\n",
    "\n",
    "        # get all note indexes \n",
    "        node_indexes = list(range(self.get_node_count()))\n",
    "\n",
    "        # start training\n",
    "        self.train_nodes(\n",
    "            node_indexes=node_indexes, \n",
    "            epochs=epochs, \n",
    "            reset=True, \n",
    "            log=True\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
